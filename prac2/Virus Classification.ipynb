{
 "metadata": {
  "name": "",
  "signature": "sha256:fff69229151e98795619f0ee19990190e7ad7b53452f6425e8dc4ef118ac921a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This file provides starter code for extracting features from the xml files and\n",
      "## for doing some learning.\n",
      "##\n",
      "## The basic set-up: \n",
      "## ----------------\n",
      "## main() will run code to extract features, learn, and make predictions.\n",
      "## \n",
      "## extract_feats() is called by main(), and it will iterate through the \n",
      "## train/test directories and parse each xml file into an xml.etree.ElementTree, \n",
      "## which is a standard python object used to represent an xml file in memory.\n",
      "## (More information about xml.etree.ElementTree objects can be found here:\n",
      "## http://docs.python.org/2/library/xml.etree.elementtree.html\n",
      "## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)\n",
      "## It will then use a series of \"feature-functions\" that you will write/modify\n",
      "## in order to extract dictionaries of features from each ElementTree object.\n",
      "## Finally, it will produce an N x D sparse design matrix containing the union\n",
      "## of the features contained in the dictionaries produced by your \"feature-functions.\"\n",
      "## This matrix can then be plugged into your learning algorithm.\n",
      "##\n",
      "## The learning and prediction parts of main() are largely left to you, though\n",
      "## it does contain code that randomly picks class-specific weights and predicts\n",
      "## the class with the weights that give the highest score. If your prediction\n",
      "## algorithm involves class-specific weights, you should, of course, learn \n",
      "## these class-specific weights in a more intelligent way.\n",
      "##\n",
      "## Feature-functions:\n",
      "## --------------------\n",
      "## \"feature-functions\" are functions that take an ElementTree object representing\n",
      "## an xml file (which contains, among other things, the sequence of system calls a\n",
      "## piece of potential malware has made), and returns a dictionary mapping feature names to \n",
      "## their respective numeric values. \n",
      "## For instance, a simple feature-function might map a system call history to the\n",
      "## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating\n",
      "## whether the first system call made by the executable was 'load_image'. \n",
      "## Real-valued or count-based features can of course also be defined in this way. \n",
      "## Because this feature-function will be run over ElementTree objects for each \n",
      "## software execution history instance, we will have the (different)\n",
      "## feature values of this feature for each history, and these values will make up \n",
      "## one of the columns in our final design matrix.\n",
      "## Of course, multiple features can be defined within a single dictionary, and in\n",
      "## the end all the dictionaries returned by feature functions (for a particular\n",
      "## training example) will be unioned, so we can collect all the feature values \n",
      "## associated with that particular instance.\n",
      "##\n",
      "## Two example feature-functions, first_last_system_call_feats() and \n",
      "## system_call_count_feats(), are defined below.\n",
      "## The first of these functions indicates what the first and last system-calls \n",
      "## made by an executable are, and the second records the total number of system\n",
      "## calls made by an executable.\n",
      "##\n",
      "## What you need to do:\n",
      "## --------------------\n",
      "## 1. Write new feature-functions (or modify the example feature-functions) to\n",
      "## extract useful features for this prediction task.\n",
      "## 2. Implement an algorithm to learn from the design matrix produced, and to\n",
      "## make predictions on unseen data. Naive code for these two steps is provided\n",
      "## below, and marked by TODOs.\n",
      "##\n",
      "## Computational Caveat\n",
      "## --------------------\n",
      "## Because the biggest of any of the xml files is only around 35MB, the code below \n",
      "## will parse an entire xml file and store it in memory, compute features, and\n",
      "## then get rid of it before parsing the next one. Storing the biggest of the files \n",
      "## in memory should require at most 200MB or so, which should be no problem for\n",
      "## reasonably modern laptops. If this is too much, however, you can lower the\n",
      "## memory requirement by using ElementTree.iterparse(), which does parsing in\n",
      "## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\n",
      "## for an example. \n",
      "\n",
      "import os\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "import numpy as np\n",
      "\n",
      "# used for data manipulation\n",
      "from scipy import sparse\n",
      "from scipy import stats\n",
      "\n",
      "import json\n",
      "\n",
      "# used to create bag of words feature\n",
      "import sklearn.feature_extraction\n",
      "\n",
      "# used for dataframe manipulation\n",
      "import pandas as pd\n",
      "\n",
      "import util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feats_single_file(ffs, direc=\"train\", virus=None):\n",
      "    '''\n",
      "    arguments:\n",
      "        ffs are a list of feature-functions.\n",
      "        direct is a directory containing xml files (expected to be train or test).\n",
      "        \n",
      "    returns:\n",
      "        a dictionary with the features for the single virus file. The file is selected\n",
      "        randomly if virus is None, otherwise the specified virus type is used\n",
      "    '''\n",
      "    for datafile in os.listdir(direc):\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        if virus is None or virus == clazz:\n",
      "            break\n",
      "            \n",
      "    tree = ET.parse(os.path.join(direc,datafile))\n",
      "    return [ff(tree) for ff in ffs]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = []\n",
      "    for datafile in os.listdir(direc):\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"X\"\n",
      "            classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_design_mat(fds, global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix((np.array(data),\n",
      "                   (np.array(rows), np.array(cols))),\n",
      "                   shape=(len(fds), len(feat_dict)))\n",
      "    return X, feat_dict\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "def first_last_system_call_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def system_call_count_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
      "      made by an executable (summed over all processes)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tag_counts_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "        tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "        a dictionary mapping 'tag' to the number of times 'tag' appears in the .xml file\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    for el in tree.iter():\n",
      "        # count the tags as you see them\n",
      "        c[el.tag + \"_count\"] += 1\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def two_sequences_feat(tree):\n",
      "    '''\n",
      "    Read sequences of length 2 from a tree.\n",
      "    '''\n",
      "    c = Counter()\n",
      "    previous_el_tag = ''\n",
      "    first_el = True\n",
      "    for el in tree.iter():\n",
      "        first_el = False\n",
      "        if (not first_el):\n",
      "            c[el.tag + '+' + previous_el_tag] += 1\n",
      "            previous_el_tag = el.tag\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lcs_feat(tree):\n",
      "    c = Counter()\n",
      "    list = []\n",
      "    for el in tree.iter():\n",
      "        list.append(el.tag)\n",
      "    return list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_tag_list_of_class(clazz):\n",
      "    list_of_lists = []\n",
      "    for datafile in os.listdir(\"train\"):\n",
      "        id_str,claz = datafile.split('.')[:2]\n",
      "        if (claz == clazz):\n",
      "            tree = ET.parse(os.path.join(\"train\", datafile))\n",
      "            list = []\n",
      "            for el in tree.iter():\n",
      "                list.append(el.tag)\n",
      "            list_of_lists.append(list)\n",
      "    return list_of_lists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 224,
       "text": [
        "'FraudPack'"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import os\n",
      "for i in xrange()\n",
      "os.path.commonprefix(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "[3, 2, 1]"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_feats_single_file([tag_counts_feats])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[Counter({'query_value': 242, 'load_dll': 135, 'open_key': 96, 'enum_window': 51, 'destroy_window': 42, 'vm_protect': 36, 'open_file': 15, 'get_system_directory': 14, 'get_file_attributes': 12, 'create_window': 11, 'create_mutex': 7, 'find_file': 6, 'thread': 6, 'all_section': 6, 'show_window': 6, 'enum_keys': 4, 'create_directory': 4, 'process': 3, 'sleep': 3, 'load_image': 3, 'get_windows_directory': 3, 'check_for_debugger': 2, 'kill_process': 2, 'set_windows_hook': 2, 'create_file': 2, 'set_file_attributes': 2, 'set_file_time': 2, 'create_thread': 2, 'com_create_instance': 2, 'open_process': 2, 'com_get_class_object': 1, 'create_process': 1, 'find_window': 1, 'enum_values': 1, 'processes': 1})]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words_feat(tree, frequency=False, count=True, subset=10):\n",
      "    \"\"\"\n",
      "    arguments: same as always boys\n",
      "        frequency = returns a dictionary with frequencies if True\n",
      "        count = returns a dictionary with counts if True\n",
      "        subset = returns only the :subset: most frequent words and least frequent words\n",
      "    returns:\n",
      "        a dictionary mapping 'word' to the frequency of that word in the file\n",
      "    \"\"\"\n",
      "    # we want to extract words using regular expressions\n",
      "    import re\n",
      "    \n",
      "    root = tree.getroot()\n",
      "    text = ET.tostring(root)\n",
      "    split_text = re.findall(r\"[\\w']+\", text)\n",
      "    cv = sklearn.feature_extraction.text.CountVectorizer(split_text)\n",
      "    cv.fit_transform(split_text)\n",
      "    \n",
      "    # count words in counter (positive and negative)\n",
      "    vocab = Counter({key: cv.vocabulary_[key] for key in cv.vocabulary_.keys() if cv.vocabulary_[key]}) if count else {}\n",
      "    vocabNeg = Counter({key: -value for key,value in vocab.items()})\n",
      "    \n",
      "    # subset accordingly\n",
      "    vocabMost = {key + \"_most\" : value for key,value in vocab.most_common(subset)} if subset > 0 else vocab\n",
      "    vocabLeast = {key + \"_least\": -value for key,value in vocabNeg.most_common(subset)} if subset > 0 else vocab\n",
      "    \n",
      "    # count frequencies\n",
      "    totalMFreq = sum(vocabMost.values())\n",
      "    totalLFreq = sum(vocabLeast.values())\n",
      "    \n",
      "    # frequency dictionaries\n",
      "    freq_dict = {key + \"_freq\" :(value/totalMFreq) for key,value in vocabMost.items()} if frequency else {}\n",
      "    least_freq_dict = {key + \"_freq\" : (value/totalLFreq) for key, value in vocabLeast.items()} if frequency else {}\n",
      "    \n",
      "    return dict(vocabMost.items() + vocabLeast.items() + freq_dict.items() + least_freq_dict.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = extract_feats_single_file([bag_of_words_feat])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main(outputfile = \"mypredictions.csv\"):\n",
      "    train_dir = \"train\"\n",
      "    test_dir = \"test\"\n",
      "    \n",
      "    # TODO put the names of the feature functions you've defined above in this list\n",
      "    ffs = [first_last_system_call_feats, system_call_count_feats]\n",
      "    \n",
      "    # extract features\n",
      "    print \"extracting training features...\"\n",
      "    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
      "    print \"done extracting training features\"\n",
      "    print\n",
      "    \n",
      "    # TODO train here, and learn your classification parameters\n",
      "    print \"learning...\"\n",
      "    learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\n",
      "    print \"done learning\"\n",
      "    print\n",
      "    \n",
      "    # get rid of training data and load test data\n",
      "    del X_train\n",
      "    del t_train\n",
      "    del train_ids\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = np.argmax(X_test.dot(learned_W),axis=1)\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_feat_extraction(feature_function, setdir = \"train\"):\n",
      "    '''\n",
      "    arguments:\n",
      "        feature function to run on data for data extraction on the training_set\n",
      "    returns:\n",
      "        extracted features\n",
      "    '''\n",
      "    print \"testing feature extraction\"\n",
      "    X_train, global_feat_dict, t_train, train_ids = extract_feats([feature_function], setdir)\n",
      "    \n",
      "    return X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_and_load(featurefile, train_dir, ffs = None, global_feat_dict=None):\n",
      "    # load features from textfile if possible (so we don't waste time recalculating this!)\n",
      "    if ffs is None:\n",
      "        print \"loading features from file: {}\".format(featurefile)\n",
      "        X_train = util.load_sparse_csr(featurefile + \"_mat.npz\")\n",
      "        global_feat_dict = json.load(open(featurefile + \"_dict.save\")) if global_feat_dict is None else global_feat_dict\n",
      "        t_train = np.load(featurefile + \"_t_train.npy\")\n",
      "        train_ids = np.load(featurefile + \"_train_ids.npy\")\n",
      "        print \"loaded features\"\n",
      "        return X_train, global_feat_dict, t_train, train_ids\n",
      "    else:\n",
      "        print \"generating feature set and saving to file: {}\".format(featurefile)\n",
      "        X_train, global_feat_dict, t_train, train_ids = extract_feats(ffs, train_dir, global_feat_dict)\n",
      "        json.dump(global_feat_dict, open(featurefile + \"_dict.save\", \"w\"))\n",
      "        np.save(featurefile + \"_train_ids\", train_ids)\n",
      "        np.save(featurefile + \"_t_train\", t_train)\n",
      "        util.save_sparse_csr(featurefile + \"_mat\", X_train)\n",
      "        print \"generated and saved features\"\n",
      "        return X_train, global_feat_dict, t_train, train_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def toPandasDataFrame(Xarray, feat_dict, classes = None):\n",
      "    '''\n",
      "    arguments:\n",
      "        a sparse numpy matrix of features\n",
      "        a dictionary mapping column indexes to column names\n",
      "        a numpy array of virus type for each element row in Xarray\n",
      "            if none, it simply does not include this information in the matrix\n",
      "        \n",
      "    returns:\n",
      "        a pandas dataframe with all features and a final column 'class'\n",
      "        specifying the virus TYPE as discussed in the spec\n",
      "    '''\n",
      "    data = pd.DataFrame(data=Xarray.toarray(), columns=feat_dict)\n",
      "    if classes is not None: \n",
      "        data['class'] = pd.Series(classes)\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TYPES = 15 # this includes None\n",
      "\n",
      "def calculateNormalParams(data, category):\n",
      "    subset = data.loc[data['class'] == category, :'class']\n",
      "    subset.drop('class', axis=1, inplace=True)\n",
      "    \n",
      "    return (subset.cov(), subset.mean())\n",
      "\n",
      "def generative_model(ffs = None, featurefile=\"generative_features\", testfeaturefile=\"test_generative_features\", outputfile = \"generative_predictions.csv\", train_dir=\"train\", test_dir=\"test\"):\n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "    \n",
      "    # now we need to train our model using generative baysian statistics\n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # subset by class and calculate the covariance matrices and the means\n",
      "    normal_params = [calculateNormalParams(pdFrame, i) for i in xrange(TYPES)] \n",
      "    \n",
      "    # calculate the normal distributions\n",
      "    normals = [stats.multivariate_normal(mean,cov, allow_singular=True) for cov, mean in normal_params]\n",
      "    \n",
      "    # here's our trained classifier model\n",
      "    model = lambda datum: np.argmax([normal.pdf(datum) for normal in normals])\n",
      "\n",
      "    # extract features from test data\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_, t_ignore, test_ids = save_and_load(testfeaturefile, test_dir, ffs, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "    \n",
      "    testData = toPandasDataFrame(X_test, global_feat_dict)\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = [model(list(el)) for index, el in testData.iterrows()]\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genffs = [system_call_count_feats, tag_counts_feats, first_last_system_call_feats]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generative_model(genffs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now we use this section to train a random forest model!\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# returns all powers of 2 in interval [m,n]\n",
      "def powsOf2(m,n):\n",
      "    i = 1\n",
      "    res = []\n",
      "    while i <= n:\n",
      "        if i >= m:\n",
      "            res.append(i)\n",
      "        i *= 2\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extractClass(data, column = 'class'):\n",
      "    '''\n",
      "    Extracts the class column from a data frame and returns (new_frame, class)\n",
      "    '''\n",
      "    y = data[column]\n",
      "    data.drop(column, axis=1, inplace=True)\n",
      "    return (data, y)\n",
      "\n",
      "def splitForCrossValidation(data, ratio = 3):\n",
      "    '''\n",
      "    Splits input dataframe into so that 1/ratio is training data (1-1/ratio) test data. \n",
      "    Returns a 4-tuple consisting of \n",
      "    (feature data frame for training data, type vector for training data, feature data frame for testing data, and type vector)\n",
      "    '''\n",
      "    split_point = int(1.0/ratio * len(data)) \n",
      "    training, training_y = extractClass(data.loc[:split_point])\n",
      "    validation, validation_y = extractClass(data.loc[split_point:]) \n",
      "    return (training, training_y, validation, validation_y)\n",
      "\n",
      "def random_forest_optimizer(estimators, ffs = None,featurefile=\"forest_features\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "    \n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # split into training set and validation set (training = features dataFrame, validation = features + type dataFrame)\n",
      "    training, training_y, validation, validation_y = splitForCrossValidation(pdFrame)\n",
      "    # training, training_y = extractClass(pdFrame)\n",
      "    # training, training_y, validation, validation_y = cross_validation.train_test_split(training, training_y, test_size = 0.4, random_state=0)\n",
      "    \n",
      "    # getAccuracy function returns the accuracy based on training and validation data for a single estimator\n",
      "    def getAccuracy(n):\n",
      "        # train model\n",
      "        model = RandomForestClassifier(n_estimators = n)\n",
      "        model.fit(training,training_y)\n",
      "        \n",
      "        # create predictions\n",
      "        preds = model.predict(validation)\n",
      "        \n",
      "        return float(reduce(lambda a, (pred, true_val): a + 1 if pred == true_val else a, zip(preds, validation_y), 0))/float(len(preds))\n",
      "    \n",
      "    # train on training set for each estimator\n",
      "    results = [(estimator, getAccuracy(estimator)) for estimator in estimators]\n",
      "    return (max(results, key=lambda (n,res): res), results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, global_feat_dict, t_train, train_ids = save_and_load(\"generative_features\", \"train\")\n",
      "pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: generative_features\n",
        "loaded features\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib\n",
      "import matplotlib as plt\n",
      "x = [a for (a,_) in allResults]\n",
      "y = [b for (_,b) in allResults]\n",
      "plt.figure(1)\n",
      "plt.plot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: MacOSX\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "'module' object is not callable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-119-3546435a3803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datafile = os.listdir(\"train\")[0]\n",
      "id_str,clazz = datafile.split('.')[:2]\n",
      "eg_tree = ET.parse(os.path.join(\"train\", datafile))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tag_list = []\n",
      "for el in eg_tree.iter():\n",
      "    tag_list.append(el.tag)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eg_root = eg_tree.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqffs = [system_call_count_feats, tag_counts_feats, first_last_system_call_feats, two_sequences_feat]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_forest_optimizer(estimators=powsOf2(1,1040), ffs = None, featurefile=\"forest_features\", train_dir = \"train\", test_dir = \"test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: forest_features\n",
        "loaded features\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "((512, 0.8848396501457726),\n",
        " [(1, 0.8347910592808552),\n",
        "  (2, 0.8241010689990281),\n",
        "  (4, 0.8517978620019436),\n",
        "  (8, 0.8760932944606414),\n",
        "  (16, 0.880466472303207),\n",
        "  (32, 0.8819241982507289),\n",
        "  (64, 0.8843537414965986),\n",
        "  (128, 0.8833819241982507),\n",
        "  (256, 0.8838678328474247),\n",
        "  (512, 0.8848396501457726),\n",
        "  (1024, 0.8843537414965986)])"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_forest_model(ffs = None, featurefile=\"forest_features\", testfeaturefile=\"test_forest_features\", outputfile = \"forest_predictions.csv\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    \n",
      "    # do a quick load of feature data\n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "\n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # generate random forest model\n",
      "    model = RandomForestClassifier(n_estimators = 1024)\n",
      "\n",
      "    # build a forest of trees from training set (X,y) where X = feature set, Y = target values\n",
      "    y = pdFrame['class']\n",
      "    pdFrame.drop('class', axis=1, inplace=True)\n",
      "    model.fit(pdFrame, y)\n",
      "\n",
      "    # extract features from test data\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_, t_ignore, test_ids = save_and_load(testfeaturefile, test_dir, ffs, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "\n",
      "    testData = toPandasDataFrame(X_test, global_feat_dict)\n",
      "    print(len(testData))\n",
      "\n",
      "    # make predictions on test data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = model.predict(testData)\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_forest_model()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: forest_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extracting test features..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loading features from file: test_forest_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done extracting test features\n",
        "\n",
        "3724\n",
        "making predictions...\n",
        "done making predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "writing predictions...\n",
        "done!\n"
       ]
      }
     ],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def SVM_model(ffs = None, featurefile=\"forest_features\", testfeaturefile=\"test_forest_features\", outputfile = \"SVM_predictions.csv\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    \n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "\n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # generate support vector classifier model\n",
      "    model = SVC()\n",
      "\n",
      "    # build a forest of trees from training set (X,y) where X = feature set, Y = target values\n",
      "    y = pdFrame['class']\n",
      "    pdFrame.drop('class', axis=1, inplace=True)\n",
      "    model.fit(pdFrame, y)\n",
      "\n",
      "    # extract features from test data\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_, t_ignore, test_ids = save_and_load(testfeaturefile, test_dir, ffs, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "\n",
      "    testData = toPandasDataFrame(X_test, global_feat_dict)\n",
      "    print(len(testData))\n",
      "\n",
      "    # make predictions on test data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = model.predict(testData)\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVM_model()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: forest_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extracting test features..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loading features from file: test_forest_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done extracting test features\n",
        "\n",
        "3724\n",
        "making predictions...\n",
        "done making predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "writing predictions...\n",
        "done!\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "from sklearn.svm import SVC, LinearSVC\n",
      "def SVM_optimizer(ffs = None,featurefile=\"forest_features\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "    \n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # split into training set and validation set (training = features dataFrame, validation = features + type dataFrame)\n",
      "    training, training_y, validation, validation_y = splitForCrossValidation(pdFrame)\n",
      "    \n",
      "    # getAccuracy function returns the accuracy based on training and validation data for a single estimator\n",
      "    def getAccuracy():\n",
      "        # train model\n",
      "        model = LinearSVC(C=0.01, class_weight={dict, \u2018auto\u2019})\n",
      "        model.fit(training,training_y)\n",
      "        \n",
      "        # create predictions\n",
      "        preds = model.predict(validation)\n",
      "        \n",
      "        return float(reduce(lambda a, (pred, true_val): a + 1 if pred == true_val else a, zip(preds, validation_y), 0))/float(len(preds))\n",
      "    \n",
      "    # train on training set for each estimator\n",
      "    return getAccuracy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-151-7f08df2afe27>, line 15)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-151-7f08df2afe27>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    model = LinearSVC(C=0.01, class_weight={dict, \u2018auto\u2019})\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVM_optimizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: forest_features\n",
        "loaded features\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "0.7954324586977648"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "forestffs = genffs\n",
      "# run with ffs = None and featurefile=\"generative_features\" to use same \n",
      "#    features as generative model (if already generated)\n",
      "# otherwise, run with ffs as a list of feature functions and specify the output\n",
      "#    name to store the feature files by featurefile param\n",
      "\n",
      "# this uses the features stored in forest_features files\n",
      "random_forest_model()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: forest_features\n",
        "loaded features\n",
        "extracting test features..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "loading features from file: test_forest_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done extracting test features\n",
        "\n",
        "3724\n",
        "making predictions...\n",
        "done making predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "writing predictions...\n",
        "done!\n"
       ]
      }
     ],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we're going to generate the modified bag_of_words feature dataset and train it on random forests\n",
      "forestffsWithBag = forestffs + [bag_of_words_feat]\n",
      "random_forest_model(ffs=forestffsWithBag, featurefile=\"bag_forest_features\", testfeaturefile=\"bag_test_forest_features\", outputfile=\"bag_forest_predictions.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "generating feature set and saving to file: bag_forest_features\n",
        "generated and saved features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extracting test features..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "generating feature set and saving to file: bag_test_forest_features\n",
        "generated and saved features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done extracting test features\n",
        "\n",
        "3724"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "making predictions...\n",
        "done making predictions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "writing predictions...\n",
        "done!\n"
       ]
      }
     ],
     "prompt_number": 140
    }
   ],
   "metadata": {}
  }
 ]
}