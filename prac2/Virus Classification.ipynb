{
 "metadata": {
  "name": "",
  "signature": "sha256:616367cb7402c25181c7e56c8a824d965d65a41443aa3cd912d41c06d7709183"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This file provides starter code for extracting features from the xml files and\n",
      "## for doing some learning.\n",
      "##\n",
      "## The basic set-up: \n",
      "## ----------------\n",
      "## main() will run code to extract features, learn, and make predictions.\n",
      "## \n",
      "## extract_feats() is called by main(), and it will iterate through the \n",
      "## train/test directories and parse each xml file into an xml.etree.ElementTree, \n",
      "## which is a standard python object used to represent an xml file in memory.\n",
      "## (More information about xml.etree.ElementTree objects can be found here:\n",
      "## http://docs.python.org/2/library/xml.etree.elementtree.html\n",
      "## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)\n",
      "## It will then use a series of \"feature-functions\" that you will write/modify\n",
      "## in order to extract dictionaries of features from each ElementTree object.\n",
      "## Finally, it will produce an N x D sparse design matrix containing the union\n",
      "## of the features contained in the dictionaries produced by your \"feature-functions.\"\n",
      "## This matrix can then be plugged into your learning algorithm.\n",
      "##\n",
      "## The learning and prediction parts of main() are largely left to you, though\n",
      "## it does contain code that randomly picks class-specific weights and predicts\n",
      "## the class with the weights that give the highest score. If your prediction\n",
      "## algorithm involves class-specific weights, you should, of course, learn \n",
      "## these class-specific weights in a more intelligent way.\n",
      "##\n",
      "## Feature-functions:\n",
      "## --------------------\n",
      "## \"feature-functions\" are functions that take an ElementTree object representing\n",
      "## an xml file (which contains, among other things, the sequence of system calls a\n",
      "## piece of potential malware has made), and returns a dictionary mapping feature names to \n",
      "## their respective numeric values. \n",
      "## For instance, a simple feature-function might map a system call history to the\n",
      "## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating\n",
      "## whether the first system call made by the executable was 'load_image'. \n",
      "## Real-valued or count-based features can of course also be defined in this way. \n",
      "## Because this feature-function will be run over ElementTree objects for each \n",
      "## software execution history instance, we will have the (different)\n",
      "## feature values of this feature for each history, and these values will make up \n",
      "## one of the columns in our final design matrix.\n",
      "## Of course, multiple features can be defined within a single dictionary, and in\n",
      "## the end all the dictionaries returned by feature functions (for a particular\n",
      "## training example) will be unioned, so we can collect all the feature values \n",
      "## associated with that particular instance.\n",
      "##\n",
      "## Two example feature-functions, first_last_system_call_feats() and \n",
      "## system_call_count_feats(), are defined below.\n",
      "## The first of these functions indicates what the first and last system-calls \n",
      "## made by an executable are, and the second records the total number of system\n",
      "## calls made by an executable.\n",
      "##\n",
      "## What you need to do:\n",
      "## --------------------\n",
      "## 1. Write new feature-functions (or modify the example feature-functions) to\n",
      "## extract useful features for this prediction task.\n",
      "## 2. Implement an algorithm to learn from the design matrix produced, and to\n",
      "## make predictions on unseen data. Naive code for these two steps is provided\n",
      "## below, and marked by TODOs.\n",
      "##\n",
      "## Computational Caveat\n",
      "## --------------------\n",
      "## Because the biggest of any of the xml files is only around 35MB, the code below \n",
      "## will parse an entire xml file and store it in memory, compute features, and\n",
      "## then get rid of it before parsing the next one. Storing the biggest of the files \n",
      "## in memory should require at most 200MB or so, which should be no problem for\n",
      "## reasonably modern laptops. If this is too much, however, you can lower the\n",
      "## memory requirement by using ElementTree.iterparse(), which does parsing in\n",
      "## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\n",
      "## for an example. \n",
      "\n",
      "import os\n",
      "from collections import Counter\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "import numpy as np\n",
      "\n",
      "# used for data manipulation\n",
      "from scipy import sparse\n",
      "from scipy import stats\n",
      "\n",
      "import json\n",
      "\n",
      "# used to create bag of words feature\n",
      "import sklearn.feature_extraction\n",
      "\n",
      "# used for dataframe manipulation\n",
      "import pandas as pd\n",
      "\n",
      "import util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feats_single_file(ffs, direc=\"train\", virus=None):\n",
      "    '''\n",
      "    arguments:\n",
      "        ffs are a list of feature-functions.\n",
      "        direct is a directory containing xml files (expected to be train or test).\n",
      "        \n",
      "    returns:\n",
      "        a dictionary with the features for the single virus file. The file is selected\n",
      "        randomly if virus is None, otherwise the specified virus type is used\n",
      "    '''\n",
      "    for datafile in os.listdir(direc):\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        if virus is None or virus == clazz:\n",
      "            break\n",
      "            \n",
      "    tree = ET.parse(os.path.join(direc,datafile))\n",
      "    return [ff(tree) for ff in ffs]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      ffs are a list of feature-functions.\n",
      "      direc is a directory containing xml files (expected to be train or test).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "\n",
      "    returns: \n",
      "      a sparse design matrix, a dict mapping features to column-numbers,\n",
      "      a vector of target classes, and a list of system-call-history ids in order \n",
      "      of their rows in the design matrix.\n",
      "      \n",
      "      Note: the vector of target classes returned will contain the true indices of the\n",
      "      target classes on the training data, but will contain only -1's on the test\n",
      "      data\n",
      "    \"\"\"\n",
      "    fds = [] # list of feature dicts\n",
      "    classes = []\n",
      "    ids = []\n",
      "    for datafile in os.listdir(direc):\n",
      "        # extract id and true class (if available) from filename\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        ids.append(id_str)\n",
      "        # add target class if this is training data\n",
      "        try:\n",
      "            classes.append(util.malware_classes.index(clazz))\n",
      "        except ValueError:\n",
      "            # we should only fail to find the label in our list of malware classes\n",
      "            # if this is test data, which always has an \"X\" label\n",
      "            assert clazz == \"X\"\n",
      "            classes.append(-1)\n",
      "        rowfd = {}\n",
      "        # parse file as an xml document\n",
      "        tree = ET.parse(os.path.join(direc,datafile))\n",
      "        # accumulate features\n",
      "        [rowfd.update(ff(tree)) for ff in ffs]\n",
      "        fds.append(rowfd)\n",
      "        \n",
      "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
      "    return X, feat_dict, np.array(classes), ids\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_design_mat(fds, global_feat_dict=None):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      fds is a list of feature dicts (one for each row).\n",
      "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
      "      should only be provided when extracting features from test data, so that \n",
      "      the columns of the test matrix align correctly.\n",
      "       \n",
      "    returns: \n",
      "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
      "        the union of features defined in any of the fds \n",
      "    \"\"\"\n",
      "    if global_feat_dict is None:\n",
      "        all_feats = set()\n",
      "        [all_feats.update(fd.keys()) for fd in fds]\n",
      "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
      "    else:\n",
      "        feat_dict = global_feat_dict\n",
      "        \n",
      "    cols = []\n",
      "    rows = []\n",
      "    data = []        \n",
      "    for i in xrange(len(fds)):\n",
      "        temp_cols = []\n",
      "        temp_data = []\n",
      "        for feat,val in fds[i].iteritems():\n",
      "            try:\n",
      "                # update temp_cols iff update temp_data\n",
      "                temp_cols.append(feat_dict[feat])\n",
      "                temp_data.append(val)\n",
      "            except KeyError as ex:\n",
      "                if global_feat_dict is not None:\n",
      "                    pass  # new feature in test data; nbd\n",
      "                else:\n",
      "                    raise ex\n",
      "\n",
      "        # all fd's features in the same row\n",
      "        k = len(temp_cols)\n",
      "        cols.extend(temp_cols)\n",
      "        data.extend(temp_data)\n",
      "        rows.extend([i]*k)\n",
      "\n",
      "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
      "   \n",
      "\n",
      "    X = sparse.csr_matrix((np.array(data),\n",
      "                   (np.array(rows), np.array(cols))),\n",
      "                   shape=(len(fds), len(feat_dict)))\n",
      "    return X, feat_dict\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
      "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
      "# feature-names to numeric values.\n",
      "def first_last_system_call_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
      "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
      "      (in other words, it returns a dictionary indicating what the first and \n",
      "      last system calls made by an executable were.)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    first = True # is this the first system call\n",
      "    last_call = None # keep track of last call we've seen\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            if first:\n",
      "                c[\"first_call-\"+el.tag] = 1\n",
      "                first = False\n",
      "            last_call = el.tag  # update last call seen\n",
      "            \n",
      "    # finally, mark last call seen\n",
      "    c[\"last_call-\"+last_call] = 1\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def system_call_count_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "      tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
      "      made by an executable (summed over all processes)\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element\n",
      "        if el.tag == \"all_section\" and not in_all_section:\n",
      "            in_all_section = True\n",
      "        elif el.tag == \"all_section\" and in_all_section:\n",
      "            in_all_section = False\n",
      "        elif in_all_section:\n",
      "            c['num_system_calls'] += 1\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tag_counts_feats(tree):\n",
      "    \"\"\"\n",
      "    arguments:\n",
      "        tree is an xml.etree.ElementTree object\n",
      "    returns:\n",
      "        a dictionary mapping 'tag' to the number of times 'tag' appears in the .xml file\n",
      "    \"\"\"\n",
      "    c = Counter()\n",
      "    for el in tree.iter():\n",
      "        # count the tags as you see them\n",
      "        c[el.tag] += 1\n",
      "    return c\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_feats_single_file([tag_counts_feats])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[Counter({'query_value': 242, 'load_dll': 133, 'open_key': 96, 'enum_window': 47, 'destroy_window': 42, 'vm_protect': 36, 'open_file': 15, 'get_system_directory': 14, 'create_window': 8, 'create_mutex': 8, 'get_file_attributes': 7, 'find_file': 6, 'thread': 5, 'all_section': 5, 'enum_keys': 4, 'show_window': 4, 'process': 3, 'sleep': 3, 'load_image': 3, 'open_process': 3, 'check_for_debugger': 2, 'kill_process': 2, 'set_windows_hook': 2, 'find_window': 2, 'create_thread': 2, 'com_create_instance': 2, 'get_windows_directory': 2, 'com_get_class_object': 1, 'create_process': 1, 'create_file': 1, 'set_file_attributes': 1, 'set_file_time': 1, 'enum_values': 1, 'processes': 1})]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words_feat(tree, frequency=False, count=True, subset=10):\n",
      "    \"\"\"\n",
      "    arguments: same as always boys\n",
      "        frequency = returns a dictionary with frequencies if True\n",
      "        count = returns a dictionary with counts if True\n",
      "        subset = returns only the :subset: most frequent words and least frequent words\n",
      "    returns:\n",
      "        a dictionary mapping 'word' to the frequency of that word in the file\n",
      "    \"\"\"\n",
      "    # we want to extract words using regular expressions\n",
      "    import re\n",
      "    \n",
      "    root = tree.getroot()\n",
      "    text = ET.tostring(root)\n",
      "    split_text = re.findall(r\"[\\w']+\", text)\n",
      "    cv = sklearn.feature_extraction.text.CountVectorizer(split_text)\n",
      "    cv.fit_transform(split_text)\n",
      "    \n",
      "    # count words in counter (positive and negative)\n",
      "    vocab = Counter({key: cv.vocabulary_[key] for key in cv.vocabulary_.keys() if cv.vocabulary_[key]}) if count else {}\n",
      "    vocabNeg = Counter({key: -value for key,value in vocab.items()})\n",
      "    \n",
      "    # subset accordingly\n",
      "    vocabMost = {key: value for key,value in vocab.most_common(subset)} if subset > 0 else vocab\n",
      "    vocabLeast = {key + \"_least\": -value for key,value in vocabNeg.most_common(subset)} if subset > 0 else vocab\n",
      "    \n",
      "    # count frequencies\n",
      "    totalMFreq = sum(vocabMost.values())\n",
      "    totalLFreq = sum(vocabLeast.values())\n",
      "    \n",
      "    # frequency dictionaries\n",
      "    freq_dict = {key + \"_freq\" :(value/totalMFreq) for key,value in vocabMost.items()} if frequency else {}\n",
      "    least_freq_dict = {key + \"_freq\" : (value/totalLFreq) for key, value in vocabLeast.items()} if frequency else {}\n",
      "    \n",
      "    return dict(vocabMost.items() + vocabLeast.items() + freq_dict.items() + least_freq_dict.items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = extract_feats_single_file([bag_of_words_feat])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def longest_sequence_feat(tree):\n",
      "    '''\n",
      "    For each system call, we extract the number of times it was called sequentially in the maximum number of sequential calls.\n",
      "    ''' \n",
      "    currentMax = Counter()\n",
      "    actualMax = Counter()\n",
      "    prev_tag = None\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        # ignore everything outside the \"all_section\" element because it's not a system call\n",
      "        if el.tag == \"all_section\":\n",
      "            in_all_section = True\n",
      "        elif in_all_section and el.tag == \"all_section\":\n",
      "            in_all_section = False\n",
      "            \n",
      "        if in_all_section:\n",
      "            # we're in a sequence so keep counting for this tag\n",
      "            if el.tag == prev_tag:\n",
      "                currentMax[el.tag] += 1\n",
      "            # we're encoutered a new tag, so reset prev_tag counter and update el.tag\n",
      "            else:\n",
      "                # before we reset, check to see if the actual value for prev_tag has improved\n",
      "                if currentMax[prev_tag] > actualMax[prev_tag] and prev_tag is not None:\n",
      "                    actualMax[prev_tag] = currentMax[prev_tag]\n",
      "                currentMax[prev_tag] = 0\n",
      "                prev_tag = el.tag\n",
      "                currentMax[el.tag] = 1\n",
      "\n",
      "    return actualMax"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_feats_single_file([longest_sequence_feat])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[Counter({'load_dll': 48, 'vm_protect': 32, 'create_mutex': 5, 'query_value': 4, 'open_key': 4, 'create_window': 2, 'open_file': 2, 'set_windows_hook': 2, 'get_file_attributes': 2, 'open_process': 2, 'process': 1, 'check_for_debugger': 1, 'sleep': 1, 'com_get_class_object': 1, 'kill_process': 1, 'load_image': 1, 'create_process': 1, 'enum_window': 1, 'find_file': 1, 'create_file': 1, 'find_window': 1, 'destroy_window': 1, 'set_file_attributes': 1, 'enum_keys': 1, 'create_thread': 1, 'enum_values': 1, 'set_file_time': 1, 'thread': 1, 'get_system_directory': 1, 'all_section': 1, 'com_create_instance': 1, 'show_window': 1, 'get_windows_directory': 1})]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_url_connections(tree):\n",
      "    '''\n",
      "    For each system call which is 'open_url' we extract the server attribute\n",
      "    '''\n",
      "    keywords = [\"server\", \"url\", \"open\"]\n",
      "    c = Counter()\n",
      "    in_all_section = False\n",
      "    for el in tree.iter():\n",
      "        if el.tag == \"all_section\":\n",
      "            in_all_section = True\n",
      "        elif in_all_section and el.tag == \"all_section\":\n",
      "            in_all_section = False\n",
      "            \n",
      "        # ignore everything outside the ''all_section'' element because it's not a system call\n",
      "        if in_all_section:\n",
      "            for key,value in el.attrib.iteritems():\n",
      "                if reduce(lambda a,c: a or c, [(keyword in key or keyword in value) for keyword in keywords], False):\n",
      "                    c[key + \"-\" + value] += 1\n",
      "\n",
      "    return c        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "extract_feats_single_file([extract_url_connections])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[Counter({'target-urlmon.dll.URLDownloadToFileA': 2, 'target-urlmon.dll.URLOpenBlockingStreamA': 2, 'target-urlmon.dll.URLOpenStreamW': 2, 'target-urlmon.dll.URLDownloadToCacheFileW': 2, 'target-urlmon.dll.URLOpenPullStreamW': 2, 'target-urlmon.dll.URLOpenBlockingStreamW': 2, 'target-urlmon.dll.URLOpenStreamA': 2, 'target-urlmon.dll.URLDownloadToFileW': 2, 'target-urlmon.dll.URLOpenPullStreamA': 2, 'target-urlmon.dll.URLDownloadToCacheFileA': 2, 'inprocserver32-C:\\\\WINDOWS\\\\system32\\\\urlmon.dll': 1, 'filename-C:\\\\WINDOWS\\\\system32\\\\urlmon.dll': 1, 'inprocserver32-C:\\\\WINDOWS\\\\system32\\\\ieframe.dll': 1})]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def two_sequences_feat(tree):\n",
      "    '''\n",
      "    Read sequences of length 2 from a tree.\n",
      "    '''\n",
      "    c = Counter()\n",
      "    previous_el_tag = ''\n",
      "    first_el = True\n",
      "    for el in tree.iter():\n",
      "        first_el = False\n",
      "        if (not first_el):\n",
      "            c[el.tag + '+' + previous_el_tag] += 1\n",
      "            previous_el_tag = el.tag\n",
      "    return c_k\n",
      "\n",
      "# generalization of the above\n",
      "def sequence_feat(tree, n):\n",
      "    '''\n",
      "    Reads a sequence of lenth n from a tree\n",
      "    '''\n",
      "    c = Counter()\n",
      "    previous_k = []\n",
      "    loaded_n = False\n",
      "    for el in tree.iter():\n",
      "        if loaded_n:\n",
      "            c[\"+\".join([t for t in previous_k])] += 1\n",
      "            previous_k.append(el.tag)\n",
      "            previous_k.pop(0)\n",
      "        else:\n",
      "            previous_k.append(el.tag)\n",
      "            if len(previous_k) == n:\n",
      "                loaded_n = True\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = extract_feats_single_file([lambda x: sequence_feat(x,4)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_feat_extraction(feature_function, setdir = \"train\"):\n",
      "    '''\n",
      "    arguments:\n",
      "        feature function to run on data for data extraction on the training_set\n",
      "    returns:\n",
      "        extracted features\n",
      "    '''\n",
      "    print \"testing feature extraction\"\n",
      "    X_train, global_feat_dict, t_train, train_ids = extract_feats([feature_function], setdir)\n",
      "    \n",
      "    return X\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_and_load(featurefile, train_dir, ffs = None, global_feat_dict=None):\n",
      "    # load features from textfile if possible (so we don't waste time recalculating this!)\n",
      "    if ffs is None:\n",
      "        print \"loading features from file: {}\".format(featurefile)\n",
      "        X_train = util.load_sparse_csr(featurefile + \"_mat.npz\")\n",
      "        global_feat_dict = json.load(open(featurefile + \"_dict.save\")) if global_feat_dict is None else global_feat_dict\n",
      "        t_train = np.load(featurefile + \"_t_train.npy\")\n",
      "        train_ids = np.load(featurefile + \"_train_ids.npy\")\n",
      "        print \"loaded features\"\n",
      "        return X_train, global_feat_dict, t_train, train_ids\n",
      "    else:\n",
      "        print \"generating feature set and saving to file: {}\".format(featurefile)\n",
      "        X_train, global_feat_dict, t_train, train_ids = extract_feats(ffs, train_dir, global_feat_dict)\n",
      "        json.dump(global_feat_dict, open(featurefile + \"_dict.save\", \"w\"))\n",
      "        np.save(featurefile + \"_train_ids\", train_ids)\n",
      "        np.save(featurefile + \"_t_train\", t_train)\n",
      "        util.save_sparse_csr(featurefile + \"_mat\", X_train)\n",
      "        print \"generated and saved features\"\n",
      "        return X_train, global_feat_dict, t_train, train_ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def toPandasDataFrame(Xarray, feat_dict, classes = None):\n",
      "    '''\n",
      "    arguments:\n",
      "        a sparse numpy matrix of features\n",
      "        a dictionary mapping column indexes to column names\n",
      "        a numpy array of virus type for each element row in Xarray\n",
      "            if none, it simply does not include this information in the matrix\n",
      "        \n",
      "    returns:\n",
      "        a pandas dataframe with all features and a final column 'class'\n",
      "        specifying the virus TYPE as discussed in the spec\n",
      "    '''\n",
      "    data = pd.DataFrame(data=Xarray.toarray(), columns=feat_dict)\n",
      "    if classes is not None: \n",
      "        data['class'] = pd.Series(classes)\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TYPES = 15 # this includes None\n",
      "\n",
      "def calculateNormalParams(data, category):\n",
      "    subset = data.loc[data['class'] == category, :'class']\n",
      "    subset.drop('class', axis=1, inplace=True)\n",
      "    \n",
      "    return (subset.cov(), subset.mean())\n",
      "\n",
      "def generative_model(ffs = None, featurefile=\"generative_features\", testfeaturefile=\"test_generative_features\", outputfile = \"generative_predictions.csv\", train_dir=\"train\", test_dir=\"test\"):\n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "    \n",
      "    # now we need to train our model using generative baysian statistics\n",
      "    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # subset by class and calculate the covariance matrices and the means\n",
      "    normal_params = [calculateNormalParams(pdFrame, i) for i in xrange(TYPES)] \n",
      "    \n",
      "    # calculate the normal distributions\n",
      "    normals = [stats.multivariate_normal(mean,cov, allow_singular=True) for cov, mean in normal_params]\n",
      "    \n",
      "    # here's our trained classifier model\n",
      "    model = lambda datum: np.argmax([normal.pdf(datum) for normal in normals])\n",
      "\n",
      "    # extract features from test data\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_, t_ignore, test_ids = save_and_load(testfeaturefile, test_dir, ffs, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "    \n",
      "    testData = toPandasDataFrame(X_test, global_feat_dict)\n",
      "    \n",
      "    # TODO make predictions on text data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = [model(list(el)) for index, el in testData.iterrows()]\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\"\n",
      "    \n",
      "    return preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genffs = [system_call_count_feats, tag_counts_feats, first_last_system_call_feats]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "generative_model(featurefile=\"expert_features2\", testfeaturefile=\"expert_features2\", outputfile=\"expert_generative2\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: expert_features2\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "MemoryError()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-23-6d701441089a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerative_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturefile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"expert_features2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestfeaturefile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"expert_features2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"expert_generative2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-20-0863b89b8227>\u001b[0m in \u001b[0;36mgenerative_model\u001b[1;34m(ffs, featurefile, testfeaturefile, outputfile, train_dir, test_dir)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# subset by class and calculate the covariance matrices and the means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mnormal_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcalculateNormalParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# calculate the normal distributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-20-0863b89b8227>\u001b[0m in \u001b[0;36mcalculateNormalParams\u001b[1;34m(data, category)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculateNormalParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1318\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0m_is_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1212\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1214\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: MemoryError()"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now we use this section to train a random forest model!\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# returns all powers of 2 in interval [m,n]\n",
      "def powsOf2(m,n):\n",
      "    i = 1\n",
      "    res = []\n",
      "    while i <= n:\n",
      "        if i >= m:\n",
      "            res.append(i)\n",
      "        i *= 2\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extractClass(data, column = 'class'):\n",
      "    '''\n",
      "    Extracts the class column from a data frame and returns (new_frame, class)\n",
      "    '''\n",
      "    y = data[column]\n",
      "    data.drop(column, axis=1, inplace=True)\n",
      "    return (data, y)\n",
      "\n",
      "def splitForCrossValidation(data, ratio = 3):\n",
      "    '''\n",
      "    Splits input dataframe into so that 1/ratio is training data (1-1/ratio) test data. \n",
      "    Returns a 4-tuple consisting of \n",
      "    (feature data frame for training data, type vector for training data, feature data frame for testing data, and type vector)\n",
      "    '''\n",
      "    split_point = int(1.0/ratio * len(data)) \n",
      "    training, training_y = extractClass(data.loc[:split_point])\n",
      "    validation, validation_y = extractClass(data.loc[split_point:]) \n",
      "    return (training, training_y, validation, validation_y)\n",
      "\n",
      "def splitForCrossValidation2(X, y, ratio =3):\n",
      "    split_point = int(1.0/ratio * X.shape[0])\n",
      "    training, training_y = X[:split_point], y[:split_point]\n",
      "    validation, validation_y = X[split_point:], y[split_point:]\n",
      "    return (training.toarray(), training_y, validation.toarray(), validation_y)\n",
      "\n",
      "def random_forest_optimizer(estimators, ffs = None,featurefile=\"forest_features\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "    \n",
      "    # pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "    \n",
      "    # split into training set and validation set (training = features dataFrame, validation = features + type dataFrame)\n",
      "    training, training_y, validation, validation_y = splitForCrossValidation2(X_train, t_train)\n",
      "    del(X_train)\n",
      "    \n",
      "    # getAccuracy function returns the accuracy based on training and validation data for a single estimator\n",
      "    def getAccuracy(n):\n",
      "        # train model\n",
      "        model = RandomForestClassifier(n_estimators = n, max_features=64)\n",
      "        model.fit(training,training_y)\n",
      "        \n",
      "        # create predictions\n",
      "        preds = model.predict(validation)\n",
      "        \n",
      "        return float(reduce(lambda a, (pred, true_val): a + 1 if pred == true_val else a, zip(preds, validation_y), 0))/float(len(preds))\n",
      "    \n",
      "    # train on training set for each estimator\n",
      "    results = [(estimator, getAccuracy(estimator)) for estimator in estimators]\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_forest_optimizer([512,1024,2048,4096], featurefile=\"expert_features3\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: expert_features3\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "[(512, 0.8984450923226434),\n",
        " (1024, 0.8989310009718173),\n",
        " (2048, 0.8979591836734694),\n",
        " (4096, 0.8979591836734694)]"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bestFeatureFunctions = [system_call_count_feats, tag_counts_feats, first_last_system_call_feats, longest_sequence_feat, \n",
      "                        extract_url_connections, lambda x: sequence_feat(x, 1), lambda  x: sequence_feat(x, 1), \n",
      "                        lambda  x: sequence_feat(x, 2), lambda  x: sequence_feat(x, 3), lambda  x: sequence_feat(x, 4),\n",
      "                        lambda x: sequence_feat(x,5), lambda x: sequence_feat(x,6)]\n",
      "X_train, global_feat_dict, t_train, train_ids = save_and_load(\"expert_features3\", \"train\", ffs=bestFeatureFunctions)\n",
      "#pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)\n",
      "\n",
      "X_train_test, _, t_test, test_ids = save_and_load(\"expert_test_features3\", \"test\", ffs=bestFeatureFunctions, global_feat_dict=global_feat_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "generating feature set and saving to file: expert_features3\n",
        "generated and saved features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "generating feature set and saving to file: expert_test_features3\n",
        "generated and saved features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "math.sqrt(len(global_feat_dict))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'global_feat_dict' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-23-8d4701708e5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_feat_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'global_feat_dict' is not defined"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdFrame = toPandasDataFrame(X_train, global_feat_dict)\n",
      "del(pdFrame)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "MemoryError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-24-9e55944a6c70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpdFrame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoPandasDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_feat_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-19-b8a665d4def9>\u001b[0m in \u001b[0;36mtoPandasDataFrame\u001b[1;34m(Xarray, feat_dict, classes)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mspecifying\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvirus\u001b[0m \u001b[0mTYPE\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdiscussed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     '''\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeat_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/luis/.virtualenvs/cs181/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/luis/.virtualenvs/cs181/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/luis/.virtualenvs/cs181/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mMemoryError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_forest_optimizer([128,512,1024,2048,4096], featurefile=\"expert_features2\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: expert_features2\n",
        "loaded features"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parameters = range(1,10) + powsOf2(10,2048)\n",
      "(optimalN, accuracy), allResults = random_forest_optimizer(parameters, featurefile=\"generative_features\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: generative_features\n",
        "loaded features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "allResults"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 101,
       "text": [
        "[(1, 0.8328474246841594),\n",
        " (2, 0.8304178814382897),\n",
        " (3, 0.8469387755102041),\n",
        " (4, 0.8551992225461613),\n",
        " (5, 0.8654033041788144),\n",
        " (6, 0.8663751214771623),\n",
        " (7, 0.8624878522837707),\n",
        " (8, 0.8688046647230321),\n",
        " (9, 0.8707482993197279),\n",
        " (16, 0.8790087463556852),\n",
        " (32, 0.8756073858114675),\n",
        " (64, 0.8794946550048591),\n",
        " (128, 0.8785228377065112),\n",
        " (256, 0.8770651117589893),\n",
        " (512, 0.879980563654033),\n",
        " (1024, 0.8785228377065112),\n",
        " (2048, 0.880466472303207)]"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib\n",
      "import matplotlib as plt\n",
      "x = [a for (a,_) in allResults]\n",
      "y = [b for (_,b) in allResults]\n",
      "plt.figure(1)\n",
      "plt.plot(x,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: MacOSX\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "'module' object is not callable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-119-3546435a3803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def random_forest_model(ffs = None, featurefile=\"forest_features\", testfeaturefile=\"test_forest_features\", outputfile = \"forest_predictions.csv\", train_dir = \"train\", test_dir = \"test\"):\n",
      "    \n",
      "    # do a quick load of feature data \n",
      "    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)\n",
      "\n",
      "    # pdFrame = toPandasDataFrame(X_train, global_feat_dict)\n",
      "    \n",
      "    # generate random forest model\n",
      "    model = RandomForestClassifier(n_estimators = 2700, max_features=300)\n",
      "\n",
      "    # build a forest of trees from training set (X,y) where X = feature set, Y = target values\n",
      "    y = t_train\n",
      "    model.fit(X_train.toarray(), y)\n",
      "    del(X_train)\n",
      "    del(y)\n",
      "\n",
      "    # extract features from test data\n",
      "    print \"extracting test features...\"\n",
      "    X_test,_, t_ignore, test_ids = save_and_load(testfeaturefile, test_dir, ffs, global_feat_dict=global_feat_dict)\n",
      "    print \"done extracting test features\"\n",
      "    print\n",
      "\n",
      "    # testData = toPandasDataFrame(X_test, global_feat_dict)\n",
      "    print(X_test.shape[0])\n",
      "\n",
      "    # make predictions on test data and write them out\n",
      "    print \"making predictions...\"\n",
      "    preds = model.predict(X_test.toarray())\n",
      "    print \"done making predictions\"\n",
      "    print\n",
      "    del(X_test)\n",
      "    print(\"Predictions\" + str(len(preds)))\n",
      "    \n",
      "    print \"writing predictions...\"\n",
      "    util.write_predictions(preds, test_ids, outputfile)\n",
      "    print \"done!\"\n",
      "    \n",
      "    return preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# forestffs = [system_call_count_feats, tag_counts_feats, first_last_system_call_feats]\n",
      "# run with ffs = None and featurefile=\"generative_features\" to use same \n",
      "#    features as generative model (if already generated)\n",
      "# otherwise, run with ffs as a list of feature functions and specify the output\n",
      "#    name to store the feature files by featurefile param\n",
      "preds = random_forest_model(featurefile=\"expert_features3\", testfeaturefile=\"expert_test_features3\", outputfile=\"expert3_features.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading features from file: expert_features3\n",
        "loaded features"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(preds == 12)/float((len(preds)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "0.13909774436090225"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import shuffle\n",
      "\n",
      "def split_data(train_dir):\n",
      "    '''\n",
      "    Splits train_dir into training set and validation set with validation size 1/3 of total size\n",
      "    Returns: train_data (1/5 of size) and validation_data (2/5 of size)\n",
      "    '''\n",
      "    train_data = os.listdir(train_dir)\n",
      "    shuffe(train_data)\n",
      "    validation_size = len(train_data) / 5\n",
      "    validation_data = train_data[0:validation_size]\n",
      "    train_data = train_data[validation_size:]\n",
      "    return (train_data, validation_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_accuracy(validation_dir, preds):\n",
      "    for i in len(validation_dir):\n",
      "        accuracy_count = 0\n",
      "        # extract id and true class from filename\n",
      "        datafile = validation_dir[i]\n",
      "        id_str,clazz = datafile.split('.')[:2]\n",
      "        # increment accuracy count if actual clazz matches predictions\n",
      "        if (util.malware_classes.index(clazz) == preds[i]):\n",
      "            accuracy_count += 1\n",
      "        return accuracy_count / len(preds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}