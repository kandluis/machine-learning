\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amssymb,amsthm,fullpage}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage{newunicodechar}
\usepackage{multicol}
\usepackage[margin={1.5cm,1.5cm}]{geometry}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
% \pdfinfo{
% /Title (Predicting the Efficiency of Organic Photovoltaics)
% /Author (Kevin K. Eskici, Luis A. Perez, Aidi Zhang)}
\setcounter{secnumdepth}{0}

% we want to adjust the seperationg between columns
\setlength{\columnsep}{1cm}

\title{Predicting the Efficiency 
of Organic Photovoltaics: \\
Computationally Efficient Machine Learning Methods
}
\author{Kevin Eskici\thanks{keskici@college.harvard.edu}, Luis A. Perez\thanks{luisperez@college.harvard.edu}, Aidi Zhang\thanks{aidizhang@college.harvard.edu} \\
Kaggle Team: cacheMoney \\
Computer Science, Harvard University}

\date{\today} 

\begin{document}

\maketitle

\begin{abstract}
\begin{quote}

\end{quote}
\end{abstract}

\section{Introduction}
Machine learning on molecules is a unique and interesting problem because of the complexity of molecular structure. Whereas quantum calculations and empirical research might take days to derive desired properties on a single problem, machine learning techniques present a faster and more efficient way of predicting molecular properties.\\
\\
Finding a computationally efficient method of predicting the HUMO/LUMO gap, for example, could lead to developments in organic photovoltaics. However, a majority of standard regression techniques model target variables given real-valued input predictor variables, and it is not immediately clear how best to represent entire molecules as inputs. Even if an accurate representation is found, the question remains of what features of our representation are the best predictors. Finally, even after the features are selected and modified in a way suitable to capture accurate predictors, the model of problem selection and optimzation remain.\\
\\
In this writeup, we consider the pros and cons of various representations of molecules. To start, we were given the standard 256-bit bit-vector extracted from RDKit Morgan Fingerprint \footnote{standard Chemistry library for the Python}. The Morgan Fingerpring of a  but an important improvement involved choosing a better feature space. We will also consider the dilemma of feature selection vs feature extraction, as well as the computational consideration associated with each.

\section{Methodology}
 For computations, and in particular new feature extraction as discussed later in the text, we utilized three seperate machines, layed out in the following table.
 
The machines were placed into an ad-hoc distributed system by seperating the large training and test sets into smaller chunks. Originally containing 

The primary off-the-shelve libraries used for computation purposes invovled 

% LINEAR REGRESSION OLS
\subsection{Linear Regression}
In the simplest linear regression approach, we consider linear basis function models that minimizes the sum-of-squares error function. To determine whether a regression function $f(x) = x^{T} \omega$ is a good candidate, we have to ask whether $\omega$ is close to the true $\omega$, as well as whether $f(x)$ will fit future observations well. Good estimators should, on average, have small prediction errors; the prediction error at a specific input value $x$ is given by:

\begin{center}
$$ E_D [ (y(x;D) - h(x))^2 ]$$
$$= E_D[ [y(x;D)] - h(x)]^2 $$
$$+ E_D [ (y(x;D)-E_D[y(x;D)])^2 ] $$
\end{center} 

\noindent The first term and second term point to the bias and variance of the regression respectively. Introducing a little bias in our estimate of $\omega$ might lead to a substantial decrease in variance of our linear coefficients, and hence a substantial decrease in prediction error.\\
\\
\noindent We can write the error function rewritten in matrix form as such:

\begin{equation}
\begin{split}
E(\omega) &= arg \min_{\omega} \frac{1}{2} \sum_{i=0}^m \left( y_i - \omega_0 - \sum_{j=1}^{n} \omega_j x_{ij} \right)^2 \\
&= arg \min_{\omega} \frac{1}{2} (y - X\omega)^{T} (y - X\omega)
\end{split}
\end{equation}

\noindent We can obtain a closed-form solution by differentiating E with respect to $\omega$, obtaining:

$$\frac{\partial E}{\partial \omega} = X^{T}(y-X\omega) = 0 \Rightarrow \omega_{opt} = (X^{T}X)^{-1} X^{T}y$$

\noindent However, Ordinary Least Squares linear regression is susceptible to over-fitting of model. Subsequently, we explore Ridge and Lasso Regression, both of which seek to alleviate the consequences of multicollinearity among predictor variables. 


% RIDGE REGRESSION
\subsection{Ridge Regression}
If unconstrained, our regression coefficients might blow up and be susceptible to high variance. For example, when variables are highly correlated, a large coefficient in one variable might be counteracted with a large coefficient in another, which is negatively correlated with the former.\\ 
\\
\noindent We want to use regularization to put an upper threshold on the values taken by coefficients. Regularization is a simple way to avoid overfitting and often improves the prediction accuracy on new data. Our new residual sum of squares error function has an addition of a penalty term on coefficients that looks like this:

$$E(\omega) = \frac{1}{2} \sum_{n=1}^N (y(x_n,w) - t_n)^2 + \frac{\lambda}{2} \lVert w \rVert ^2$$

\noindent {\bf Derivation of Ridge Regression}
Similar to how we made the conversion to matrix representation and solved for the closed form solution for the Ordinary Least Squares approach above, we get

\begin{equation}
\begin{split}
E(\omega) &= arg \min_{\omega} \frac{1}{2} (y - X\omega)^{T} (y - X\omega) + \frac{\lambda}{2} \omega^{T} \omega \\
& \Rightarrow \frac{\partial E}{\partial \omega} = X^{T}(y-X\omega) + \lambda \omega = 0 \\
&\Rightarrow \omega_{opt} = (X^{T}X + \lambda I)^{-1} X^{T}y
\end{split}
\end{equation}

\subsection{Lasso Regression}
Whereas Ridge Regression tries to re-scale the Ordinary Least Squares coefficients, the Lasso tries to produce a sparse solution in the sense that many of the coefficients will be set to zero or near-zero. The error function has a $L_1$ regularization term that looks like this:

$$E(\omega) = \frac{1}{2} \sum_{n=1}^N (y(x_n,w) - t_n)^2 + \lambda \lVert w \rVert$$
\noindent {\bf Derivation of Lasso}\\
The $L_1$ regularization instead of $L_2$ regularization in Lasso means that there is no closed form solution and makes the solution non-linear in $y_i$'s. The solution can however be efficiently approximated by solving a quadratic programming problem, and hence there is no closed form solution for Lasso.\\
\\
Scikit implements Lasso using the Least Angle Regression (LARS) algorithm, which works like this: first we start with all coefficients equal to zero, and find the predictor variable that is most correlated with the output vector $t$, say $x_i$. We take the largest step possible in the direction of this predictor until some other predictor $x_j$ becomes just as correlated to the current residual. LARS then proceeds in a direction equiangular between $x_i$ and $x_j$ until some third predictor $x_k$ becomes just as correlated to the newly updated residual as well. That is, LARS always proceeds equiangularly between the predictors that have been selected into a set of the most correlated.

\subsection{Choosing Hyperparameters: Cross-Validation}
Cross-validation was systematically used to find appropriate model parameters that minimizes the error function. After we compute our statistical model with our training set, we need a way to show that it will also perform well on a new, independent set of data. To achieve this, we used K-fold cross validation, which partitions the training set into K subsets of equal size. For each $k = 1,2,...,K$, fit the model to the training set excluding the $k$th fold and compute the cross-validation error for the $k$th fold. The model then has overall cross-validation error that is the mean of these $k$ results. \\
\\
\noindent For Ridge Regression, $\frac{\lambda}{2}$ is the complexity parameter that controls the amount of shrinkage: the larger $\lambda$ is, the lower the degree of over-fitting and the more robust the coefficients are to collinearity. To find the optimal $\lambda$, we used RidgeCV which uses cross-validation to find the optimal $\lambda$ parameter.\\
\\
Below is the table of $\lambda$ coefficients and their corresponding root mean square errors (RMSE):\\


%\noindent Ridge regression performs pretty well and results in low prediction errors because of its relationship with principle component analysis; in fact, the derived variable ...\\




\subsection{Feature Extraction vs Selection}

Feature extraction involves transforming the existing features into a lower dimensional space; feature selection involves selecting a subset of the existing features without a transformation. One of the most promising methods of feature extraction we tried was {\bf Principle Component Analysis (PCA)}. The motivation behind PCA is that the initial set of features that we were given possibly correlated variables, and we want to transform them into a set of linearly uncorrelated, orthogonal variables called principle components. \\
\\
Feature selection, on the other hand, searches for a subset of the given that minimizes some objective function, be it Bayesian Information Criterion (BIC), Alkaike Information Criterion (AIC) or Mallow's Cp statistic. We would then run Sequential Forward, Backward or Stepwise Selection to obtain the subset that optimizes the objective function. However, an exhaustive search of feature subsets involves $\binom{n}{m}$ combinations.



\subsection{Nearest Neighbors}
This idea was pretty tangential to the other methods we were working on, but ended up producing the best results we reached with the initial feature set. While doing exploratory analysis on the training data, we noticed that of the 256 features provided, a full 225 had ZERO variance among the set of molecules we were given, effectively reducing our feature set to 31. Now $2^{31}$ potential bit vectors for a molecule is substantially larger than the number of molecules in our training data (around a factor of 2000 more), so if features were randomly distributed, we would not have expected many exact matches in our dataset. But in the real world things can't always be iid. A little more poking around resulted in us finding out that among the million molecules in our training set, there was less than 5000 unique feature vectors! 

This led to us trying to come up with ways to take advantage of this peculiar structure of the data. Indeed for an examined portion of the test set, almost all of the molecules had hundreds of matches in the training data, and those that didn't had several almost matches. If limited to our original feature set, what could possible be a better predicter of a test molecule's HOMO-LUMO gap than the average HOMO-LUMO gap of all molecules in the test data with the exact same features? From this observation, a nearest neighbors approach felt natural. We defined a distance metric between two molecules as the count of the number of pairwise differences in their feature vectors. Unfortunately, finding the closest match for a molecule meant having to compute this distance a million times. Doing this for $800k+$ test molecules results in over 800 Billion operations! Timing how long this took for a small amount of test molecules and extrapolating, we found that it would take one of our computers on the order of 7-42 YEARS to finish. Now we had a couple options- we abandon methods (but we were so curious about the results this method would yield), ask Professor Adams really nicely if we could delay the Kaggle deadline until 2057, or (wait for it..) redefine our distance metric. After some hard debate between the second two options, we went for the latter.

Whereas it takes hundreds of operations to count the number of differences in 2 arrays with over 250 elements each, it only takes one to find the difference between two numbers.

\subsection{New Feature Ideas}
The original set of 256 features given to us was extracted from the RDKit Morgan Fingerprint converted to bit vectors. Out of the 256 features, 225 of them had zero variance, i.e. all the bits were 0's or all the bits were 1's. Thus, since these features provide no extra value to train our model (for regression weights assigned to these features would just be absorbed into the $w_0$ term), we deleted them from our dataframe to reduce computational complexity. An important observation was that no matter how sophisticated our model was, there would be a cap on performance if we did not have sufficiently predictive features.\\
\\
We thus conducted research into molecular properties that might be important predictors of HOMO-LUMO gap. 

However, domain-specific knowledge 
extracting a large set of features 
Principle Component Analysis

\section{Results and Discussion}

\section{Further Development}
\subsection{Random Forests}




\end{document}
























